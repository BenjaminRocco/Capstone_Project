# The Detailed Explanation of Labeling Functions

### Use of Regular Expressions, Length Functions, and Sentiment Analysis

All of our labeling functions took in as input a string of text and as output a $0$ if it did not meet the criteria for the label and a $1$ if they did meet that criteria. In this document, we discuss in more detail the protocol for labeling these passages. We utilize `Snorkel` for labeling functions and `TextBlob` for sentiment analysis. 

In order to label our text and create categories based on the features and content of the passages, we need a way to delineate characters, words, and phrases from one another and ways to capture the general feeling that are associated with these forms. To accomplish this on a fundamental level, we make use of Regular Expressions. 

Regular Expressions, Regex for short, is a system in coding that identifies characters of string literals and can parse through words, phrases, and paragraphs based on defined criteria designated by the user. When combined with a coding language to host these fundamental character forms, regular expressions and native coding language functions are combined to derive meaning from text and manipulate it in ways for it to be analyzed for future use. 

In this project, Regex is utilized to remove common words (known in the language processing community as "stop words") from text that would not contribute any meaningful insight regarding the passages (think: words like ‘the’, ‘that’, ‘is’, etc.) Next, Regex is actually combined with our labeling functions to further isolate words of interest that can potentially lead to a biased article. These keywords have been determined by examining the most common words that occurred in abstracts/headline false positive/false negatives from a binary classification model trained to identify articles that were in the `Opinion` section of The New York Times versus articles that were contained in all other sections, as a starting point. During the train-test-split, the binary classification model had $100\%$ train and $88\%$ test accuracy, specificity of $95\%$, an $F1$ score of $72 \%$, and a misclassification rate of a mere $12\%$. Therefore, it performed quite well at this task and the passages that it had trouble identifying contained vocabulary that was justifiable for use in a labeling function. Similar label functions were created based on words that invoke sentiments in readers (‘feel’, ’think’, ‘believe’ etc.). Throughout the project, Regex was utilized for similar preprocessing tasks.  

Calculating the average length of passages and using this as an indicator for bias was the inspiration behind yet another labeling function utilized in this project. If the length of the passage was less than the average length among all passages, around $130$ characters, the function would be labeled as ‘longer.’ Longer abstracts/headlines tend to result in more bias as they generally have longer abstracts which contain more flowery language and more feeling invoking words than shorter abstracts. 

Finally, sentiment analysis using TextBlob was the source of more labeling functions for this project. Positive articles (given by polarity score) tend to be more biased, contrary to popular belief, and a subjective score correlates directly with bias (more subjective more opinionated).

### Specific Snorkel Metrics

Snorkel has unique metrics that are returned and may be used for further analysis. These metrics are $\textbf{Polarity}$: The set of unique labels this LF outputs (excluding abstains). $\textbf{Coverage}$: The fraction of the dataset the LF labels. $\textbf{Overlaps}$: The fraction of the dataset where this LF and at least one other LF labels. $\textbf{Conflicts}$: The fraction of the dataset where this LF and at least one other LF label and disagree. $\textbf{Correct}$: The number of data points this LF labels correctly (if gold labels are provided). $\textbf{Incorrect}$: The number of data points this LF labels incorrectly (if gold labels are provided). $\textbf{Empirical Accuracy}$: The empirical accuracy of this LF (if gold labels are provided). Only Polarity, Coverage, Overlaps, and Conflicts were surveyed in this project as gold labels were not provided. Only UNBIASED (denoted $0$) versus BIASED (denoted $1$) were label returns; ABSTAIN was not included. A small dataset of $100$ entries was handlabeled by the author based on notions of bias outlined within the study and this was used as the validation set for a training set test. The results of this were marginal at best, yielding $62\%$ test accuracy, thus, further handlabeling with larger datasets and more stringent conditions will be required in future iterations of this study. 

### The Cohen Kappa Score Output of The Labeling Function Linear Equation

Apart from the Snorkel file and assessing those metrics for performance, the binary labeling functions were compiled into a single linear equation with appropriate scalar weights. Since all of these labeling functions would output either $0$ or $1$, and associated with each of these functions was a unique variable, the coefficients of these variables were weights that would place higher importance on functions with greater connection to bias (subjectivity functions and those with buzzwords etc.). This linear equation was normalized to return a number ranging from $0$ to $1$. If this number fell within $0-0.2$, the text was said to be not-to-slightly-biased. $0.2-0.4$ slightly-to-moderately biased. $0.4-0.6$ moderately-to-pretty-biased. $0.6-0.8$ pretty-to-mostly-biased. $0.8-1.0$ perfectly-biased. These were the tiers for our rating system in this project. 

These ranges are characteristic for a Cohen Kappa score, which is a statistical measure used to quantify the level of agreement between two raters (voters, judges, observers, etc.) who each classify items into categories. In this case, each of these labeling functions serves as a voter, a judge, a rater, a reviewer. The equation is the amalgamation of their opinions. The return value is the Cohen Kappa score and collective representation of the level of bias that each voter believed any given article contained. Now that we have detailed the protocol for these labeling functions, we delve into the analysis behind this work in `Summary_of_Findings.md` for a general audience and `Technical_Findings.md` for the more technical audience. 







